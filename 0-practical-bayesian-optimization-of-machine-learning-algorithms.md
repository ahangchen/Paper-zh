# 机器学习算法贝叶斯优化实践
## 摘要
使用机器学习算法经常需要小心地调参（学习参数或者模型参数）。不幸的是，这种调节往往是需要专业经验，运气，或者一些暴力搜索，这些东西构成的黑魔法。因此，能够优化任何给出的学习算法的自动调参有着巨大的吸引力，在这篇文章中，我们在贝叶斯优化的角度考虑这个问题，学习算法泛化的性能由高斯过程来模拟。我们说明了几种高斯过程本质，比如内核类型，和对超参数的处理，可能在获取一个好的优化器这个问题上可以达到专家级的表现。我们描述了新的算法，考虑变量在学习算法实验中的代价（周期），这可以在多核并行实验作为表征。我们展示了这些合适的算法优化了先前的自动化过程，并且在许多算法上达到甚至超过了人类专家级优化，包裹LDA，SVM，CNN。
## 介绍
机器学习算法几乎没有参数无关的：参数控制来学习率，模型的容量，都需要指定。通常认为这些参数是一种麻烦。人们倾向于发展拥有更少参数的机器学习算法。另一方面，这个话题另一种灵活的方式是，将超参数的优化视为一个自动化过程。特别的，我们可以将这种优化视为一个未知黑箱函数，然后调用处理这种问题的算法。一个好的选择是贝叶斯优化，已经在大量挑战性的优化问题上证明它比其他全局优化算法更佳。对于连续函数，贝叶斯优化通常通过假设未知函数被一个高斯过程筛选过并且为这个函数维护一个后分布作为观察，或者，在我们的例子里，用不同的参数运行学习算法实验。为了挑选下一个实验的参数，我们可以在当前最好结果上优化期望提高（EI）或者优化高斯过程的上置信度（UCB）。EI和UCB在大量寻找全局最优问题的多模黑箱函数评估问题上被证明有效。

机器学习算法，然而，有着特定的与其他黑箱优化问题不同的特征。首先，每个函数评估需要大量的时间，训练一个有着10个隐含单元的小型神经网络需要的时间比一个更大的有着1000个隐含单元的神经网络消耗更少的时间。即使不考虑耗时，云计算的出现使得需要的大内存学习机器，改变了不同隐含单元所需的实际经济价值。其次，机器学习实验通常是并行的，多核或多机的。在这些情况中，标准顺序GP优化过程可能不理想。

在这篇文章中，我们实践了好的机器学习算法贝叶斯优化。我们证明，基于GP超参数的优化比基于GP内核的优化要更好，正如前面所说的。我们第二个贡献是描述了新的算法，考虑了变量和实验的未知代价，以及多核可用性以并行运行实验。

高斯过程已经被证明在计算实验的代理模型上有效，基于此，合适性分析，标准化和预测的实践被建立起来。这些策略在优化的时候却不被考虑，实际上，这些对于机器学习研究者理解不同参数时模型的有效性很有帮助。哈特已经发展了一个顺序的基于模型的优化策略，以以设置满意度，用随机森林混合整数编程。然而我们考虑的机器学习算法，构造一个完全贝叶斯手段，因他们昂贵的自然必要性以最小化评估的时间。贝叶斯优化策略也被用于调节蒙特卡洛算法的超参数。最近本格斯特拉已经探索了大量优化机器学习算法的策略。他们证明网格搜索策略比随机搜索差，建议使用高斯过程贝叶斯优化以优化一个平方到指数的分布，和TPA。
